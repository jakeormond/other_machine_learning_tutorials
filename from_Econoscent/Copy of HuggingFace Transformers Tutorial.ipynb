{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"182fculAfvkSqwYb-QM5eNJmJCBcmiRVJ","timestamp":1706383443369}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdGG86vbnkdZ","executionInfo":{"status":"ok","timestamp":1706382282121,"user_tz":0,"elapsed":38342,"user":{"displayName":"John Ormond","userId":"02502530184509941369"}},"outputId":"9420c8e7-2879-429b-f281-221ff51eae71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.26.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"]}],"source":["! pip install -U accelerate\n","! pip install -U transformers\n","\n","# ! pip install transformers\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset"]},{"cell_type":"code","source":["class AdditionDataset(Dataset):\n","    \"\"\"\n","    Returns addition problems of up to some number of digits in the inputs. Recall\n","    that all GPT cares about are sequences of integers, and completing them according to\n","    patterns in the data. Therefore, we have to somehow encode addition problems\n","    as a sequence of integers.\n","\n","    The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n","    encoding will simply be the n-digit first number, n-digit second number,\n","    and (n+1)-digit result, all simply concatenated together. Because each addition\n","    problem is so structured, there is no need to bother the model with encoding\n","    +, =, or other tokens. Each possible sequence has the same length, and simply\n","    contains the raw digits of the addition problem.\n","\n","    As a few examples, the 2-digit problems:\n","    - 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5]\n","    - 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5]\n","    etc.\n","\n","    We will also only train GPT on the final (n+1)-digits because the first\n","    two n-digits are always assumed to be given. So when we give GPT an exam later,\n","    we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\n","    to add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5]\n","    in 3 sequential steps.\n","\n","    fun exercise: does it help if the result is asked to be produced in reverse order?\n","    \"\"\"\n","\n","    def __init__(self, ndigit, split):\n","        self.split = split # train/test\n","        self.ndigit = ndigit\n","        self.vocab_size = 10 # 10 possible digits 0..9\n","        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n","        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n","\n","        # split up all addition problems into either training data or test data\n","        num = (10**self.ndigit)**2 # total number of possible combinations\n","        r = np.random.RandomState(1337) # make deterministic\n","        perm = r.permutation(num)\n","        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n","        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n","\n","    def __len__(self):\n","        return self.ixes.size\n","\n","    def __getitem__(self, idx):\n","        # given a problem index idx, first recover the associated a + b\n","        idx = self.ixes[idx]\n","        nd = 10**self.ndigit\n","        a = idx // nd\n","        b = idx %  nd\n","        c = a + b\n","        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\"\n","\n","        return {\"input_ids\":tokenizer(render[:-3])[\"input_ids\"], \"labels\":tokenizer(render[-3:])[\"input_ids\"]}"],"metadata":{"id":"8iqM4VL6dBQh","executionInfo":{"status":"ok","timestamp":1706382333039,"user_tz":0,"elapsed":207,"user":{"displayName":"John Ormond","userId":"02502530184509941369"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["model_checkpoint = \"t5-small\"\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","tokenizer([\"0\", \"01\", \"20\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XTQbAUoqacK7","executionInfo":{"status":"ok","timestamp":1706382341474,"user_tz":0,"elapsed":1668,"user":{"displayName":"John Ormond","userId":"02502530184509941369"}},"outputId":"8716cd58-7d7d-4419-af2d-3ad8285b6d27"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[3, 632, 1], [7088, 1], [460, 1]], 'attention_mask': [[1, 1, 1], [1, 1], [1, 1]]}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["batch_size = 128\n","model_name = model_checkpoint.split(\"/\")[-1]\n","args = Seq2SeqTrainingArguments(\n","    f\"{model_name}-finetuned-xsum\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-3,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=50,\n","    predict_with_generate=True,\n","    push_to_hub=False,\n",")\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"],"metadata":{"id":"DdG8aL_ScjXu","executionInfo":{"status":"ok","timestamp":1706382484847,"user_tz":0,"elapsed":214,"user":{"displayName":"John Ormond","userId":"02502530184509941369"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=AdditionDataset(2,\"train\"),\n","    eval_dataset=AdditionDataset(2,\"test\"),\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")\n","\n","trainer.train()"],"metadata":{"id":"O3uSMIN9ctCg","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1706382957677,"user_tz":0,"elapsed":436155,"user":{"displayName":"John Ormond","userId":"02502530184509941369"}},"outputId":"cee93fcd-9f77-47a1-853b-73700d95eb87"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3550' max='3550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3550/3550 07:14, Epoch 50/50]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.238322</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.047816</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.949335</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.895559</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>0.814020</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>No log</td>\n","      <td>0.763756</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>No log</td>\n","      <td>0.691789</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.079900</td>\n","      <td>0.682255</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.079900</td>\n","      <td>0.609168</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.079900</td>\n","      <td>0.583301</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>1.079900</td>\n","      <td>0.551103</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>1.079900</td>\n","      <td>0.522311</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>1.079900</td>\n","      <td>0.491903</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>1.079900</td>\n","      <td>0.480570</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.628900</td>\n","      <td>0.453688</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.628900</td>\n","      <td>0.439451</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.628900</td>\n","      <td>0.444413</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.628900</td>\n","      <td>0.423720</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.628900</td>\n","      <td>0.399256</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.628900</td>\n","      <td>0.401358</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.628900</td>\n","      <td>0.375280</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.469500</td>\n","      <td>0.368007</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.469500</td>\n","      <td>0.359481</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.469500</td>\n","      <td>0.340347</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.469500</td>\n","      <td>0.350504</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.469500</td>\n","      <td>0.309610</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.469500</td>\n","      <td>0.293938</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.469500</td>\n","      <td>0.272277</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.363300</td>\n","      <td>0.261934</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.363300</td>\n","      <td>0.258441</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.363300</td>\n","      <td>0.219552</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.363300</td>\n","      <td>0.203742</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.363300</td>\n","      <td>0.187584</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.363300</td>\n","      <td>0.173509</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.363300</td>\n","      <td>0.170141</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.239600</td>\n","      <td>0.158193</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.239600</td>\n","      <td>0.131079</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.239600</td>\n","      <td>0.124325</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.239600</td>\n","      <td>0.111776</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.239600</td>\n","      <td>0.105505</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.239600</td>\n","      <td>0.098439</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.239600</td>\n","      <td>0.091082</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.141900</td>\n","      <td>0.079393</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.141900</td>\n","      <td>0.076592</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.141900</td>\n","      <td>0.067775</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.141900</td>\n","      <td>0.064630</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.141900</td>\n","      <td>0.062197</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.141900</td>\n","      <td>0.059825</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.141900</td>\n","      <td>0.058648</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.084600</td>\n","      <td>0.058509</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=3550, training_loss=0.42466637006947694, metrics={'train_runtime': 435.1876, 'train_samples_per_second': 1034.037, 'train_steps_per_second': 8.157, 'total_flos': 475811020800000.0, 'train_loss': 0.42466637006947694, 'epoch': 50.0})"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["test = AdditionDataset(2,\"test\")\n","count = 0\n","for index in range(len(test)):\n","  device = 'cuda'\n","  preds = model.generate(input_ids = torch.tensor(test[index][\"input_ids\"]).to(device).view(1,-1))\n","  count+=1.0*(int(tokenizer.decode(np.array(preds.cpu()[0]))[5:-4]) == int(tokenizer.decode(np.array(test[index][\"labels\"]))[:-4]))\n","  if index%10==0:\n","    print(tokenizer.decode(np.array(preds.cpu()[0]))[5:-4],tokenizer.decode(np.array(test[index][\"labels\"]))[:-4])\n","count/len(test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"hi-v9TLbe4u8","executionInfo":{"status":"error","timestamp":1706382997140,"user_tz":0,"elapsed":23645,"user":{"displayName":"John Ormond","userId":"02502530184509941369"}},"outputId":"2099e948-fd6e-4e1b-e8f3-f9ea5d21f275"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" 126 126\n"," 104 104\n"," 072 072\n"," 104 104\n"," 027 027\n"," 062 062\n"," 014 014\n"," 167 168\n"," 100 100\n"," 062 062\n"," 082 082\n"," 057 057\n"," 174 174\n"," 123 123\n"," 061 061\n"," 123 123\n"," 027 027\n"," 072 072\n"," 139 139\n"," 069 069\n"," 089 089\n"," 036 036\n"," 130 130\n"," 142 142\n"," 067 067\n"," 122 122\n"," 049 049\n"," 089 089\n"," 112 112\n"," 067 067\n"," 007 007\n"," 121 121\n"," 060 060\n"," 138 138\n"," 137 137\n"," 109 109\n"," 110 110\n"," 096 096\n"," 106 106\n"," 084 084\n"," 063 063\n"," 010 010\n"," 101 101\n"," 032 032\n"," 056 056\n"," 157 157\n"," 160 160\n"," 125 125\n"," 104 104\n"," 132 132\n"," 057 057\n"," 135 135\n"," 034 034\n"," 099 099\n"," 124 124\n"," 021 021\n"," 051 051\n"," 146 146\n"," 108 108\n"," 086 086\n"]},{"output_type":"error","ename":"ValueError","evalue":"invalid literal for int() with base 10: ' 03 2020'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-0eb3c68172da>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ' 03 2020'"]}]},{"cell_type":"code","source":["type(model)"],"metadata":{"id":"_JHzbLzVzYz7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706383262410,"user_tz":0,"elapsed":196,"user":{"displayName":"John Ormond","userId":"02502530184509941369"}},"outputId":"92971d6a-0be4-464b-82ab-64c056d2c72d"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["transformers.models.t5.modeling_t5.T5ForConditionalGeneration"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["! zip -r mathformer t5-small-finetuned-xsum/"],"metadata":{"id":"NflM1Zg3YQ6D","executionInfo":{"status":"ok","timestamp":1646423270924,"user_tz":480,"elapsed":122346,"user":{"displayName":"Vaishak V Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRAXzyXWJQXdyirvuhVAIsPYNN_JtB863UFNb5kw=s64","userId":"17677299833973929582"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c378ca54-250f-4ae1-d3e7-0fbc705b7ab3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: t5-small-finetuned-xsum/ (stored 0%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3500/ (stored 0%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3500/tokenizer_config.json (deflated 80%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3500/tokenizer.json (deflated 74%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3500/rng_state.pth (deflated 27%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3500/trainer_state.json (deflated 82%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3500/pytorch_model.bin (deflated 9%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3500/special_tokens_map.json (deflated 83%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3500/config.json (deflated 62%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3500/scheduler.pt (deflated 49%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3500/optimizer.pt (deflated 6%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3500/training_args.bin (deflated 48%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3000/ (stored 0%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3000/tokenizer_config.json (deflated 80%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3000/tokenizer.json (deflated 74%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3000/rng_state.pth (deflated 27%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3000/trainer_state.json (deflated 82%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3000/pytorch_model.bin (deflated 9%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3000/special_tokens_map.json (deflated 83%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3000/config.json (deflated 62%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3000/scheduler.pt (deflated 49%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3000/optimizer.pt (deflated 6%)\n","  adding: t5-small-finetuned-xsum/checkpoint-3000/training_args.bin (deflated 48%)\n","  adding: t5-small-finetuned-xsum/checkpoint-2500/ (stored 0%)\n","  adding: t5-small-finetuned-xsum/checkpoint-2500/tokenizer_config.json (deflated 80%)\n","  adding: t5-small-finetuned-xsum/checkpoint-2500/tokenizer.json (deflated 74%)\n","  adding: t5-small-finetuned-xsum/checkpoint-2500/rng_state.pth (deflated 27%)\n","  adding: t5-small-finetuned-xsum/checkpoint-2500/trainer_state.json (deflated 81%)\n","  adding: t5-small-finetuned-xsum/checkpoint-2500/pytorch_model.bin (deflated 9%)\n","  adding: t5-small-finetuned-xsum/checkpoint-2500/special_tokens_map.json (deflated 83%)\n","  adding: t5-small-finetuned-xsum/checkpoint-2500/config.json (deflated 62%)\n","  adding: t5-small-finetuned-xsum/checkpoint-2500/scheduler.pt (deflated 49%)\n","  adding: t5-small-finetuned-xsum/checkpoint-2500/optimizer.pt (deflated 7%)\n","  adding: t5-small-finetuned-xsum/checkpoint-2500/training_args.bin (deflated 48%)\n","  adding: t5-small-finetuned-xsum/runs/ (stored 0%)\n","  adding: t5-small-finetuned-xsum/runs/Mar04_19-31-08_69686e1de8b6/ (stored 0%)\n","  adding: t5-small-finetuned-xsum/runs/Mar04_19-31-08_69686e1de8b6/1646422279.2545087/ (stored 0%)\n","  adding: t5-small-finetuned-xsum/runs/Mar04_19-31-08_69686e1de8b6/1646422279.2545087/events.out.tfevents.1646422279.69686e1de8b6.79.1 (deflated 62%)\n","  adding: t5-small-finetuned-xsum/runs/Mar04_19-31-08_69686e1de8b6/events.out.tfevents.1646422279.69686e1de8b6.79.0 (deflated 66%)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/drive')"],"metadata":{"id":"0UyQQ6cpZwV9","executionInfo":{"status":"ok","timestamp":1646423316032,"user_tz":480,"elapsed":45117,"user":{"displayName":"Vaishak V Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRAXzyXWJQXdyirvuhVAIsPYNN_JtB863UFNb5kw=s64","userId":"17677299833973929582"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"53b01959-3359-4a3c-dcf1-c77bf8dcedc1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /drive\n"]}]},{"cell_type":"code","source":["!cp mathformer.zip /drive/MyDrive/BACHU/mathformer.zip"],"metadata":{"id":"fCL1rB0VaIbF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls /drive/MyDrive/BACHU/"],"metadata":{"id":"3K3dYmv3aut3","executionInfo":{"status":"ok","timestamp":1646423349545,"user_tz":480,"elapsed":27,"user":{"displayName":"Vaishak V Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRAXzyXWJQXdyirvuhVAIsPYNN_JtB863UFNb5kw=s64","userId":"17677299833973929582"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"83837a11-fa6b-422d-a018-b4d5c3a5feed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mathformer.zip\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"MLS_M3CdZvOT"},"execution_count":null,"outputs":[]}]}